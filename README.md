
This code is the real-time decoding (RTD) software meant to be used on any computing platform that can run Python, and which receives input from the cFaCES or similar device as described in the following article: https://www.nature.com/articles/s41551-020-00612-w
First, gather training data by running rt_calibrate_motions.py in the terminal, following and answering the terminal prompts.
Next, perform real-time classification by running rt_main.py in the terminal, following and answering the terminal prompts.
All other files are support files and there is no need to run those in the terminal.

-------------------------------------------------------------------------------------
When laminated on the facial skin, this low-cost, mass-manufacturable cFaCES thus enables the creation of a library of motions from which a large subset of human language could possibly be inferred. The size of this subset depends on the method of mapping facial motions to language as well as the number of distinct facial motions chosen for decoding. The final number of motions chosen for decoding will depend on the number of phrases or ideas desired to be communicated as well as the chosen mapping strategy. Different strategies for this mapping, i.e. direct, tree, conditional, affect the total number of possible ideas or actions the user can communicate using seven motions. The motion library can be created by each user, based on their preferences and comfort. Each motion can be classified as one of the motions in the library via a real-time decoding (RTD) algorithm, which uses a dynamic-time warping, K-nearest neighbors (KNN-DTW) model. The KNN-DTW algorithm predicts the most likely motion based on calculating the distance between each of the voltage waveforms detected during testing with all the detected waveforms in the training set for the model. Motion classification relies on calculating the distance between sets of voltage waveforms from two motions. It is important to note that the KNN-DTW algorithm effectively compares the voltage waveform shapes rather than voltage values or principal-component analysis. For each detected motion, $$n$$ voltage waveforms are captured, each one corresponding to a particular sensing element on cFaCES. Distances $$d_i$$ between the voltage waveforms corresponding to the same element $$i$$ are calculated, and the root-mean square value is then calculated to get the total distance, $$d = \sqrt{\sum_{i=1}^{n} (d_i)^2}$$, between an observed signal and a signal in the training set, where $$n$$ is total number of sensing elements. Each distance $$d_i$$ is calculated from DTW, and specifically an approximation for DTW which coarsens temporal resolution of the voltage waveform, computes a warped distance matrix between two signals at that lower resolution, and projects that matrix back into finer resolution. This algorithm has previously been developed and built into a Python library called fastdtw 62, which was adapted for use in this KNN-DTW model. Once the total distance between the detected motion and each of the motions in the training set is calculated, the $$K$$ nearest neighbors, i.e. voltage waveform sets with the $$K$$ lowest total distances, are identified and their motion labels are used to determine the weighted-average probabilities of each motion label. The motion with the highest probability is dubbed the “classification” of the detected motion. 

In the RTD system, a cFaCES is laminated on the facial skin and connected to a custom-built signal-processing board (SPB) that performs differential voltage amplification, analog signal filtering, and analog-to-digital conversion, and whose output is connected to a Raspberry Pi (RPi), for portable applications. The onboard processor on the RPi runs the KNN-DTW algorithm code files, which are written in the Python language (v3.6). Before RTD can be tested, the subject completes a training session which involves performing a motion 12 times. Each motion is identified in the code, and the four-second interval containing the motion-induced voltage output (100 Hz sampling rate) of the sensor is detected, stored, and filtered digitally using a 6th order low-pass Butterworth filter with 6 Hz cutoff frequency. This is repeated for each distinct motion type in the subject’s desired library, and forms the calibration set. The subject then performs a series of motions which are classified in real-time using the KNN-DTW algorithm (Movie S8). The extent of the “real-time” aspect of our decoding algorithm is qualified by the average lag time between the end of the user’s performance of the motion and the display of the classification result, which is 1.71 ± 0.12 s. Evaluation of the RTD system on healthy and ALS subjects involved measuring the testing accuracy. Eight voltage waveforms are randomly selected from each motion in the calibration set, forming the training set. Testing accuracy refers to the percentage of motions correctly identified from the remaining 4 voltage waveforms from each motion in the calibration set when predicted using a model containing the training set. Testing accuracies reported here are calculated upon evaluating the RTD model using 3-fold stratified cross validation, resulting in 27 different data set combinations for each combination of piezoelectric elements usable for model evaluation.
